{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dropout, Flatten, Dense, BatchNormalization, Activation, GlobalAveragePooling1D, Add, LSTM, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score, hamming_loss\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "\n",
    "\n",
    "\n",
    "class ModelFactory:\n",
    "\n",
    "    class DataPreparation:\n",
    "        \n",
    "        def __init__(self):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Initializes the class and prepares a dictionary to store fitted encoders.\n",
    "\n",
    "            Parameters: \n",
    "                None\n",
    "            \n",
    "            Returns: \n",
    "                None\n",
    "            \"\"\"\n",
    "            self.encoders={}\n",
    "            pass\n",
    "\n",
    "        def _sort(self, df):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Cleans column names, converts data types, removes the Simtime column if it exists, and sorts the DataFrame by Timestamp and Machine.\n",
    "\n",
    "            Parameters:\n",
    "                df (pd.DataFrame): Raw input data.\n",
    "\n",
    "            Returns:\n",
    "                df_sorted (pd.DataFrame): Cleaned and sorted DataFrame.\n",
    "\n",
    "            Shape change: \n",
    "                input shape: 9\n",
    "                output shape: 8\n",
    "                Index is reset.\n",
    "            \"\"\"\n",
    "            df = self._clean_columns(df)\n",
    "            df = self._clean_datatypes(df)\n",
    "            if 'Simtime' in df.columns:\n",
    "                df = df.drop(columns=['Simtime'])\n",
    "            return df.sort_values(by=['Timestamp', 'Machine']).reset_index(drop=True)\n",
    "\n",
    "        def _clean_columns(self, df):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Strips whitespace and capitalizes column names.\n",
    "\n",
    "            Parameters:\n",
    "                df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "            Returns:\n",
    "                df (pd.DataFrame): DataFrame with standardized column names.\n",
    "\n",
    "            Shape change: \n",
    "                None\n",
    "            \"\"\"\n",
    "            df.columns = df.columns.str.strip().str.capitalize()\n",
    "            return df\n",
    "        \n",
    "        def _clean_datatypes(self, df):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Applies helper functions to convert date, boolean, and numeric fields to proper types.\n",
    "\n",
    "            Parameters:\n",
    "                df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "            Returns:\n",
    "                df (pd.DataFrame): Type-corrected DataFrame.\n",
    "            \n",
    "            Shape change: \n",
    "                None\n",
    "            \"\"\"\n",
    "            df = self._date_data(df)\n",
    "            df = self._boolean_data(df)\n",
    "            df = self._numeric_data(df)\n",
    "            return df\n",
    "        \n",
    "        def _date_data(self, df):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Converts Timestamp column to datetime.\n",
    "\n",
    "            Parameters:\n",
    "                df (pd.DataFrame): DataFrame with a Timestamp column.\n",
    "\n",
    "            Returns:\n",
    "                df (pd.DataFrame): Updated DataFrame with Timestamp as datetime.\n",
    "\n",
    "            Shape change: \n",
    "            None\n",
    "            \"\"\"\n",
    "            df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"])\n",
    "            return df\n",
    "        \n",
    "        def _boolean_data(self,df,target_columns=['Tool_warning', 'Tool_replaced']):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Converts specified boolean columns to integers (0 or 1).\n",
    "\n",
    "            Parameters:\n",
    "                - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "                - target_columns (list): List of column names to convert (default: ['Tool_warning', 'Tool_replaced']).\n",
    "\n",
    "            Returns:\n",
    "                df (pd.DataFrame): DataFrame with converted boolean fields.\n",
    "\n",
    "            Shape change:\n",
    "                None\n",
    "            \"\"\"\n",
    "            for col in target_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype(int)\n",
    "            return df\n",
    "        \n",
    "        def _numeric_data(self,df,target_columns=['Processed_parts', 'Scrapped_parts', 'Quality_counter']):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Converts specified numeric columns to numeric dtype, coercing errors.\n",
    "\n",
    "            Parameters:\n",
    "                - df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "                - target_columns (list): List of column names to convert (default: ['Processed_parts', 'Scrapped_parts', 'Quality_counter']).\n",
    "\n",
    "            Returns:\n",
    "                df (pd.DataFrame): Updated DataFrame with numeric values.\n",
    "\n",
    "            Shape change:\n",
    "                None\n",
    "            \"\"\"\n",
    "            for col in target_columns:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            return df\n",
    "        \n",
    "        def transform_state_columns(self, df):\n",
    "            \"\"\"\n",
    "            Description: \n",
    "                Transforms previously fitted state columns using stored OneHotEncoder objects.\n",
    "\n",
    "            Parameters:\n",
    "                df (pd.DataFrame): DataFrame with encoded categorical columns (e.g., 'MC1_State').\n",
    "\n",
    "            Returns:\n",
    "                df (pd.DataFrame): DataFrame with categorical columns replaced by one-hot encoded columns.\n",
    "\n",
    "            Shape change: \n",
    "                Adds one-hot columns and drops originals.\n",
    "\n",
    "            \"\"\"\n",
    "            df = df.copy()\n",
    "            state_cols = [col for col in df.columns if col in self.encoders]\n",
    "        \n",
    "            for col in state_cols:\n",
    "                encoder = self.encoders[col]\n",
    "                encoded = encoder.transform(df[[col]])\n",
    "        \n",
    "                categories = encoder.categories_[0]\n",
    "                encoded_col_names = [f\"{col}_{cat}\" for cat in categories]\n",
    "        \n",
    "                encoded_df = pd.DataFrame(encoded, columns=encoded_col_names, index=df.index)\n",
    "        \n",
    "                df = pd.concat([df.drop(columns=[col]), encoded_df], axis=1)\n",
    "        \n",
    "            return df\n",
    "\n",
    "        def encode_state_columns(self, df):\n",
    "            df = df.copy()\n",
    "            state_cols = [col for col in df.columns if col.endswith('_State')]\n",
    "        \n",
    "            for col in state_cols:\n",
    "                encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                encoded = encoder.fit_transform(df[[col]])\n",
    "        \n",
    "                categories = encoder.categories_[0]\n",
    "                encoded_col_names = [f\"{col}_{cat}\" for cat in categories]\n",
    "        \n",
    "                encoded_df = pd.DataFrame(encoded, columns=encoded_col_names, index=df.index)\n",
    "        \n",
    "                df = pd.concat([df.drop(columns=[col]), encoded_df], axis=1)\n",
    "        \n",
    "                self.encoders[col] = encoder\n",
    "        \n",
    "            return df\n",
    "        \n",
    "        def pivot_features(self, df, index_col='Timestamp', machine_col='Machine'):\n",
    "            df = df.copy()\n",
    "    \n",
    "            # Remove columns that shouldn't be pivoted\n",
    "            non_feature_cols = [index_col, machine_col]\n",
    "            feature_cols = [col for col in df.columns if col not in non_feature_cols]\n",
    "        \n",
    "            # Pivot to [timestamp Ã— (machine_feature)]\n",
    "            pivot_df = df.pivot(index=index_col, columns=machine_col, values=feature_cols)\n",
    "        \n",
    "            # Flatten multi-index columns: (feature, machine) â†’ 'Machine_Feature'\n",
    "            pivot_df.columns = [f\"{machine}_{feature}\" for feature, machine in pivot_df.columns]\n",
    "        \n",
    "            # Ensure Timestamp is a column again (optional)\n",
    "            pivot_df = pivot_df.reset_index()\n",
    "        \n",
    "            return pivot_df\n",
    "        \n",
    "        def _generate_labels(self, df):\n",
    "            df = df.copy()\n",
    "            df['Tool_change_label'] = 0\n",
    "            df['Quality_label'] = 0\n",
    "            for machine in df['Machine'].unique():\n",
    "                machine_df = df[df['Machine'] == machine]\n",
    "                tool_replaced_indices = machine_df.index[machine_df['Tool_replaced'] == 1]\n",
    "\n",
    "                for idx in tool_replaced_indices:\n",
    "                    prev_idx = machine_df.index[machine_df.index < idx].max()\n",
    "                    if pd.notna(prev_idx):\n",
    "                        df.at[prev_idx, 'Tool_change_label'] = 1\n",
    "\n",
    "            df.loc[df['Quality_counter'] == 1, 'Quality_label'] = 1\n",
    "\n",
    "            columns_to_drop = ['Scrapped_parts', 'Quality_counter', 'Tool_replaced']\n",
    "            df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "\n",
    "            return df\n",
    "        \n",
    "        def get_train_data(self, data):\n",
    "            \"\"\"\n",
    "            Prepares training data by sorting, labeling, pivoting, and encoding the input dataset.\n",
    "        \n",
    "            Processing steps include:\n",
    "                - Sorting the data chronologically and by machine.\n",
    "                - Generating labels based on task-specific logic.\n",
    "                - Pivoting features into a time-aligned format.\n",
    "                - Encoding machine state-related columns.\n",
    "        \n",
    "            Parameters:\n",
    "                data (pd.DataFrame): Raw input data containing timestamped machine signals.\n",
    "        \n",
    "            Returns:\n",
    "                features (pd.DataFrame): Feature set with encoded values, excluding label columns.\n",
    "                labels (pd.DataFrame): Label set containing the timestamp and task-related labels.\n",
    "                label_names (list): List of column names corresponding to the labels.\n",
    "            \"\"\"\n",
    "            data = self._sort(data)\n",
    "            data = self._generate_labels(data)\n",
    "            data = self.pivot_features(data)\n",
    "            data = self.encode_state_columns(data)\n",
    "            \n",
    "            selected_columns = []\n",
    "            for col in data.columns:\n",
    "                if col == 'Timestamp' or 'label' in col.lower():\n",
    "                    selected_columns.append(col)\n",
    "\n",
    "            labels = data[selected_columns].copy()\n",
    "            features = data.drop(columns=[col for col in selected_columns if col != 'Timestamp'])\n",
    "            \n",
    "            features = features.reset_index(drop=True)\n",
    "            labels = labels.reset_index(drop=True)\n",
    "            label_names = labels.columns.tolist()\n",
    "\n",
    "            return features, labels, label_names\n",
    "    \n",
    "        def get_predict_row(self, df):\n",
    "            data = self.sort(df)\n",
    "            columns_to_drop = ['Scrapped_parts', 'Quality_counter', 'Tool_replaced']\n",
    "            data.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n",
    "            data = self.pivot_features(data)\n",
    "            data = self.transform_state_columns(data)\n",
    "            return data.to_numpy().flatten()\n",
    "\n",
    "        def split_data(self, df, train_size,validate_size):\n",
    "            \"\"\"\n",
    "            Splits the preprocessed dataset into training, validation, and test sets.\n",
    "\n",
    "            Parameters:\n",
    "                df (pd.DataFrame): The input dataframe containing preprocessed data.\n",
    "                train_size (float): Proportion of the dataset to include in the training set (0 < train_size < 1).\n",
    "                validate_size (float): Proportion of the dataset to include in the validation set (0 < validate_size < 1).\n",
    "                    The remaining data is used for the test set.\n",
    "\n",
    "            Returns:\n",
    "                train (dict): Dictionary with keys 'X' and 'y' for training features and labels prepared.\n",
    "\n",
    "                validate (dict): Dictionary with keys 'X' and 'y' for validation features and labels prepared.\n",
    "\n",
    "                test (dict): Dictionary with keys 'X' and 'y' for test features and labels prepared.\n",
    "                \n",
    "                label_names (list): List of label column names prepared.\n",
    "            \"\"\"\n",
    "            features, labels ,label_names= self.get_train_data(df)  # preprocess first\n",
    "            n = len(features)\n",
    "        \n",
    "            train_end = int(train_size * n)\n",
    "            validate_end = int((train_size + validate_size) * n)\n",
    "        \n",
    "            X_train = features[:train_end].reset_index(drop=True)\n",
    "            y_train = labels[:train_end].reset_index(drop=True)\n",
    "            train = {'X': X_train, 'y': y_train}\n",
    "        \n",
    "            X_validate = features[train_end:validate_end].reset_index(drop=True)\n",
    "            y_validate = labels[train_end:validate_end].reset_index(drop=True)\n",
    "            validate = {'X': X_validate, 'y': y_validate}\n",
    "        \n",
    "            X_test = features[validate_end:].reset_index(drop=True)\n",
    "            y_test = labels[validate_end:].reset_index(drop=True)\n",
    "            test = {'X': X_test, 'y': y_test}\n",
    "        \n",
    "            return train, validate, test,label_names\n",
    "        \n",
    "        def split_for_continued_training(self, df):\n",
    "            \"\"\"\n",
    "            Splits data into 90% training and 10% test set for continued training purposes.\n",
    "        \n",
    "            This function is designed for scenarios where a model has already been trained\n",
    "            and is being updated with new data. It performs preprocessing using the same \n",
    "            pipeline as in initial training (label generation, encoding, pivoting, etc.).\n",
    "        \n",
    "            Parameters:\n",
    "                df (pd.DataFrame): Raw input DataFrame containing machine signal data.\n",
    "        \n",
    "            Returns:\n",
    "                train (dict): Dictionary with 'X' and 'y' for training data (90%).\n",
    "                test (dict): Dictionary with 'X' and 'y' for test data (10%).\n",
    "                label_names (list): List of label column names used in training.\n",
    "            \"\"\"\n",
    "            features, labels, label_names = self.get_train_data(df)\n",
    "            n = len(features)\n",
    "            split_idx = int(n * 0.9)\n",
    "        \n",
    "            X_train = features[:split_idx].reset_index(drop=True)\n",
    "            y_train = labels[:split_idx].reset_index(drop=True)\n",
    "            X_test = features[split_idx:].reset_index(drop=True)\n",
    "            y_test = labels[split_idx:].reset_index(drop=True)\n",
    "        \n",
    "            train = {'X': X_train, 'y': y_train}\n",
    "            test = {'X': X_test, 'y': y_test}\n",
    "        \n",
    "            return train, test, label_names\n",
    "\n",
    "        def split_yearly_data(self, df, time_column='Timestamp'):\n",
    "            \"\"\"\n",
    "            Splits a full-year DataFrame into:\n",
    "            - 3 months training data\n",
    "            - 9 months test data split into 5d, 10d, and 30d batches\n",
    "        \n",
    "            Returns:\n",
    "                {\n",
    "                    'train_3mo': DataFrame,\n",
    "                    'test_5d': list of DataFrames,\n",
    "                    'test_10d': list of DataFrames,\n",
    "                    'test_30d': list of DataFrames\n",
    "                }\n",
    "            \"\"\"\n",
    "            df = df.copy()\n",
    "            df[time_column] = pd.to_datetime(df[time_column])\n",
    "            df = df.sort_values(by=time_column).reset_index(drop=True)\n",
    "        \n",
    "            start_time = df[time_column].min()\n",
    "            train_end_time = start_time + pd.DateOffset(months=3)\n",
    "        \n",
    "            train_df = df[df[time_column] < train_end_time]\n",
    "            test_df = df[df[time_column] >= train_end_time]\n",
    "        \n",
    "            def split_batches(df, days):\n",
    "                batches = []\n",
    "                current_start = df[time_column].min()\n",
    "                current_end = current_start + pd.Timedelta(days=days)\n",
    "                while current_start < df[time_column].max():\n",
    "                    batch = df[(df[time_column] >= current_start) & (df[time_column] < current_end)]\n",
    "                    if not batch.empty:\n",
    "                        batches.append(batch)\n",
    "                    current_start = current_end\n",
    "                    current_end = current_start + pd.Timedelta(days=days)\n",
    "                return batches\n",
    "        \n",
    "            return {\n",
    "                'train': train_df,\n",
    "                '1d':split_batches(test_df,1),\n",
    "                '5d': split_batches(test_df, 5),\n",
    "                '10d': split_batches(test_df, 10),\n",
    "                '30d': split_batches(test_df, 30)\n",
    "            }\n",
    "    \n",
    "        def compute_label_weights(self, y_train, smoothing=1.0):\n",
    "            \"\"\"\n",
    "            Computes inverse frequency weights for multi-label classification.\n",
    "\n",
    "            Args:\n",
    "                y_train (pd.DataFrame or np.ndarray): Binary label matrix.\n",
    "                smoothing (float): Added to avoid division by zero.\n",
    "\n",
    "            Returns:\n",
    "                np.ndarray: Array of label weights (higher for rare labels).\n",
    "            \"\"\"\n",
    "            if isinstance(y_train, pd.DataFrame):\n",
    "                y_train = y_train.to_numpy()\n",
    "\n",
    "            label_freq = y_train.sum(axis=0)  # Number of positives per label\n",
    "            label_weights = 1.0 / (label_freq + smoothing)  # Inverse freq\n",
    "            label_weights = label_weights / label_weights.sum() * len(label_weights)  # Normalize to mean ~1\n",
    "\n",
    "            scale_tool = 7.0\n",
    "            scale_quality = 0.001\n",
    "\n",
    "            scaled_weights = label_weights.copy()\n",
    "            scaled_weights[:6] *= scale_tool\n",
    "            scaled_weights[6:] *= scale_quality\n",
    "\n",
    "            return scaled_weights\n",
    "\n",
    "    class BaseModel:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def generate_sequence(self, X, y):\n",
    "            X = X.reset_index(drop=True)\n",
    "            y = y.reset_index(drop=True)\n",
    "\n",
    "             # Drop Timestamp if present\n",
    "            if 'Timestamp' in y.columns:\n",
    "                y = y.drop(columns=['Timestamp'])\n",
    "\n",
    "            X_seq, y_seq = [], []\n",
    "            number_of_sequences = len(X) - self.window_size - self.forecast_horizon  + 1 \n",
    "            for i in range(number_of_sequences):\n",
    "                X_window = X.iloc[i:i+self.window_size].to_numpy(dtype=np.float32)\n",
    "                y_window = y.iloc[i+self.window_size + self.forecast_horizon -1].to_numpy(dtype=np.float32)\n",
    "\n",
    "                X_seq.append(X_window)\n",
    "                y_seq.append(y_window)\n",
    "            return np.array(X_seq), np.array(y_seq)\n",
    "        \n",
    "        @staticmethod\n",
    "        def safe_get(config_dict, key, default):\n",
    "            value = config_dict.get(key, default)\n",
    "            return default if value is None else value\n",
    "\n",
    "        def save(self, path_prefix):\n",
    "            if self.model:\n",
    "                self.model.save(f\"{path_prefix}_model.h5\")\n",
    "\n",
    "            # Save encoders if available\n",
    "            if self.data_preparation and hasattr(self.data_preparation, 'encoders'):\n",
    "                joblib.dump(self.data_preparation.encoders, f\"{path_prefix}_encoders.pkl\")\n",
    "\n",
    "            # Save model configuration\n",
    "            config = {\n",
    "                \"window_size\": self.window_size,\n",
    "                \"forecast_horizon\": self.forecast_horizon,\n",
    "                \"kernel_size\": self.kernel_size,\n",
    "                \"num_filters\": self.num_filters,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "                \"num_stacks\": self.num_stacks,\n",
    "                \"dilations\": self.dilations,\n",
    "                \"padding\": self.padding,\n",
    "                \"use_skip_connections\": self.use_skip_connections,\n",
    "                \"activation\": self.activation,\n",
    "                \"optimizer\": self.optimizer,\n",
    "                \"loss\": self.loss,\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"epochs\": self.epochs\n",
    "            }\n",
    "\n",
    "            with open(f\"{path_prefix}_model_info.json\", \"w\") as f:\n",
    "                json.dump(config, f, indent=4)\n",
    "        \n",
    "        def load(self, path_prefix, data_preparation):\n",
    "            self.model = load_model(f\"{path_prefix}_model.h5\")\n",
    "            encoders = joblib.load(f\"{path_prefix}_encoders.pkl\")\n",
    "            data_preparation.encoders = encoders\n",
    "            self.data_preparation = data_preparation\n",
    "\n",
    "        def predict(self, x):\n",
    "            self.history_buffer.append(x)\n",
    "            if len(self.history_buffer) > self.window_size:\n",
    "                self.history_buffer.pop(0)\n",
    "            if len(self.history_buffer) == self.window_size:\n",
    "                X_input = np.stack(self.history_buffer)[np.newaxis, ...]\n",
    "                prediction = self.model.predict(X_input, verbose=0)[0]\n",
    "                return prediction\n",
    "            return None\n",
    "\n",
    "    class LSTMModel(BaseModel):\n",
    "        def __init__(self, config=None, **kwargs):\n",
    "            config = config or {}\n",
    "            config.update(kwargs)\n",
    "\n",
    "            self.window_size = self.safe_get(config, 'window_size', 30)\n",
    "            self.forecast_horizon = self.safe_get(config, 'forecast_horizon', 1)\n",
    "            self.num_units = self.safe_get(config, 'num_units', 64)\n",
    "            self.dropout_rate = self.safe_get(config, 'dropout_rate', 0.2)\n",
    "            self.num_layers = self.safe_get(config, 'num_layers', 1)\n",
    "            self.return_sequences = self.safe_get(config, 'return_sequences', False)\n",
    "            self.bidirectional = self.safe_get(config, 'bidirectional', False)\n",
    "            self.version = self.safe_get(config, 'version', 'V1')\n",
    "\n",
    "            self.optimizer = self.safe_get(config, 'optimizer', 'adam')\n",
    "            self.loss = self.safe_get(config, 'loss', 'binary_crossentropy')\n",
    "            self.batch_size = self.safe_get(config, 'batch_size', 32)\n",
    "            self.epochs = self.safe_get(config, 'epochs', 20)\n",
    "            self.lstm_activation = self.safe_get(config, 'lstm_activation', 'tanh')  \n",
    "            self.output_activation = self.safe_get(config, 'output_activation', 'sigmoid')  \n",
    "            self.label_weights = self.safe_get(config, 'label_weights', None)\n",
    "            self.return_state = self.safe_get(config, 'return_state', False)\n",
    "\n",
    "            self.model = None\n",
    "            self.history_buffer = []\n",
    "\n",
    "        def build(self, features, labels):\n",
    "            X_seq, y_seq = self.generate_sequence(features, labels)\n",
    "            input_shape = X_seq.shape[1:]\n",
    "            num_labels = y_seq.shape[1]\n",
    "            self.V1(input_shape, num_labels)\n",
    "\n",
    "        def V1(self, input_shape, num_labels):\n",
    "            inputs = Input(shape=input_shape)\n",
    "\n",
    "            # Configure LSTM (return state for flexibility)\n",
    "            lstm = LSTM(\n",
    "                units=self.num_units,\n",
    "                return_sequences=self.return_sequences,\n",
    "                return_state=self.return_state,\n",
    "                dropout=self.dropout_rate,\n",
    "                activation=self.lstm_activation,\n",
    "                recurrent_activation='sigmoid'\n",
    "            )\n",
    "\n",
    "            if self.bidirectional:\n",
    "                lstm = Bidirectional(lstm)\n",
    "\n",
    "            # Apply LSTM layer(s)\n",
    "            outputs = lstm(inputs)\n",
    "\n",
    "            # Handle outputs based on return_state\n",
    "            if self.return_state:\n",
    "                if self.bidirectional:\n",
    "                    whole_seq_output, forward_h, forward_c, backward_h, backward_c = outputs\n",
    "                    x = tf.concat([forward_h, backward_h], axis=-1)\n",
    "                else:\n",
    "                    whole_seq_output, h, c = outputs\n",
    "                    x = h\n",
    "            else:\n",
    "                x = outputs if not self.return_sequences else outputs\n",
    "\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "            #x = Dense(num_labels, activation=self.activation)(x)\n",
    "            x = Dense(num_labels, activation=self.output_activation)(x)\n",
    "\n",
    "            model = Model(inputs, x)\n",
    "            model.compile(\n",
    "                optimizer=self.optimizer,\n",
    "                loss=self.loss,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            self.model = model\n",
    "\n",
    "\n",
    "        def predict(self, x):\n",
    "            self.history_buffer.append(x)\n",
    "            if len(self.history_buffer) > self.window_size:\n",
    "                self.history_buffer.pop(0)\n",
    "        \n",
    "            if len(self.history_buffer) == self.window_size:\n",
    "                X_input = np.stack(self.history_buffer)[np.newaxis, ...]\n",
    "                y_prob = self.model.predict(X_input, verbose=0)[0]\n",
    "                return (y_prob > 0.5).astype(int)  # Return binary predictions\n",
    "            return None\n",
    "\n",
    "\n",
    "    class TCNModel(BaseModel):\n",
    "        \n",
    "        def __init__(self, config=None, **kwargs):\n",
    "\n",
    "            config = config or {}\n",
    "            config.update(kwargs)\n",
    "\n",
    "\n",
    "            self.window_size = self.safe_get(kwargs, 'window_size', 30)\n",
    "            self.forecast_horizon = self.safe_get(kwargs, 'forecast_horizon', 144)\n",
    "            self.kernel_size = self.safe_get(kwargs, 'kernel_size', 3)\n",
    "            self.num_filters = self.safe_get(kwargs, 'num_filters', 64)\n",
    "            self.dropout_rate = self.safe_get(kwargs, 'dropout_rate', 0.2)\n",
    "            self.num_stacks = self.safe_get(kwargs, 'num_stacks', 1)\n",
    "            self.dilations = self.safe_get(kwargs, 'dilations', [1, 2, 4, 8])\n",
    "            self.padding = self.safe_get(kwargs, 'padding', 'causal')\n",
    "            self.use_skip_connections = self.safe_get(kwargs, 'use_skip_connections', True)\n",
    "            # Non-model config\n",
    "            self.verbose = self.safe_get(kwargs, 'verbose', True)\n",
    "            self.history_buffer = []\n",
    "            # Hyperparameters\n",
    "            self.activation= self.safe_get(kwargs, 'activation', 'relu')\n",
    "            self.return_sequences=self.safe_get(kwargs, 'return_sequences',False)\n",
    "            self.optimizer= self.safe_get(kwargs,'optimizer', 'adam')\n",
    "            self.loss=self.safe_get(kwargs,'loss','binary_crossentropy')\n",
    "            self.batch_size=self.safe_get(kwargs,'batch_size',32)\n",
    "            self.epochs=self.safe_get(kwargs,'epochs',20)\n",
    "            self.label_weights =self.safe_get(kwargs,'label_weights',None) \n",
    "#\n",
    "            self.model = None  # Will be built in .build_model()\n",
    "            self.history_buffer = []\n",
    "\n",
    "        def build(self, features, labels):\n",
    "            X_seq, y_seq  = self.generate_sequence(features,labels)\n",
    "            input_shape = X_seq.shape[1:]\n",
    "            print(\"TCN input_shape:\", input_shape)\n",
    "            num_labels = y_seq.shape[1]\n",
    "            return self.V3(input_shape, num_labels)\n",
    "        \n",
    "        def V1(self, input_shape, num_labels):\n",
    "            inputs = Input(shape=input_shape)\n",
    "\n",
    "            x = Conv1D(self.num_filters, kernel_size=self.kernel_size, padding=self.padding, dilation_rate=1, activation=self.activation)(inputs)\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "            x = Conv1D(self.num_filters, kernel_size=self.kernel_size, padding=self.padding, dilation_rate=2, activation=self.activation)(x)\n",
    "            x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "            x = Flatten()(x)\n",
    "            outputs = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs, outputs)\n",
    "            model.compile(optimizer=Adam(),\n",
    "                          loss=self.weighted_binary_crossentropy if self.label_weights is not None else self.loss,\n",
    "                          metrics=['accuracy'])  # You can extend this later\n",
    "\n",
    "            self.model = model\n",
    "\n",
    "        def V2(self, input_shape, num_labels):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = inputs\n",
    "        \n",
    "            # Progressive dilation stack\n",
    "            for dilation in self.dilations:\n",
    "                x = Conv1D(self.num_filters,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           padding=self.padding,\n",
    "                           dilation_rate=dilation)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Activation(self.activation)(x)\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "            # Compact output\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            outputs = Dense(num_labels, activation='sigmoid')(x)\n",
    "        \n",
    "            model = Model(inputs, outputs)\n",
    "            model.compile(optimizer=self.optimizer,\n",
    "                          loss=self.weighted_binary_crossentropy if self.label_weights is not None else self.loss,\n",
    "                          metrics=['binary_accuracy'])\n",
    "        \n",
    "            self.model = model\n",
    "    \n",
    "        def V3(self, input_shape, num_labels):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = inputs\n",
    "            skip_connections = []\n",
    "\n",
    "            for i, dilation in enumerate(self.dilations):\n",
    "                conv = Conv1D(filters=self.num_filters,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              dilation_rate=dilation,\n",
    "                              activation=self.activation)(x)\n",
    "                conv = Dropout(self.dropout_rate)(conv)\n",
    "                conv = BatchNormalization()(conv)\n",
    "                skip_connections.append(conv)\n",
    "                x = conv\n",
    "\n",
    "            if self.use_skip_connections and skip_connections:\n",
    "                x = Add()(skip_connections)\n",
    "\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            outputs = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs, outputs)\n",
    "            model.compile(optimizer=self.optimizer,\n",
    "                          loss=self.weighted_binary_crossentropy if self.label_weights is not None else self.loss,\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            self.model = model\n",
    "\n",
    "        def V4(self, input_shape, num_labels):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = inputs\n",
    "            skip_connections = []\n",
    "\n",
    "            for dilation in self.dilations:\n",
    "                residual = x\n",
    "\n",
    "                x = Conv1D(filters=self.num_filters,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           padding=self.padding,\n",
    "                           dilation_rate=dilation)(x)\n",
    "                x = BatchNormalization()(x)\n",
    "                x = Activation(self.activation)(x)\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "                if residual.shape[-1] == x.shape[-1]:\n",
    "                    x = Add()([x, residual])\n",
    "\n",
    "                skip_connections.append(x)\n",
    "\n",
    "            if self.use_skip_connections:\n",
    "                x = Add()(skip_connections)\n",
    "\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            outputs = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "            self.model = Model(inputs, outputs)\n",
    "            self.model.compile(optimizer=self.optimizer,\n",
    "                               loss=self.loss,\n",
    "                               metrics=['accuracy'])\n",
    "\n",
    "        def show(self):\n",
    "            conf = \"\"\n",
    "            conf += f\"window_size: {self.window_size} \"\n",
    "            conf += f\"forecast_horizon: {self.forecast_horizon} \"\n",
    "            conf += f\"kernel_size: {self.kernel_size} \"\n",
    "            conf += f\"num_filters: {self.num_filters} \"\n",
    "            conf += f\"dropout_rate: {self.dropout_rate} \"\n",
    "            conf += f\"num_stacks: {self.num_stacks} \"\n",
    "            conf += f\"dilations: {self.dilations} \"\n",
    "            conf += f\"padding: {self.padding} \"\n",
    "            conf += f\"use_skip_connections: {self.use_skip_connections} \"\n",
    "            conf += f\"verbose : {self.verbose }\"\n",
    "            conf += f\"activation: {self.activation}\"\n",
    "            conf += f\"return_sequences: {self.return_sequences}\"\n",
    "            conf += f\"optimizer: {self.optimizer}\"\n",
    "            conf += f\"loss: {self.loss}\"\n",
    "            conf += f\"batch_size: {self.batch_size}\"\n",
    "            conf += f\"epochs: {self.epochs}\"\n",
    "            conf += f\"label_weights: {self.label_weights}\"\n",
    "            print(conf)\n",
    "    \n",
    "    class CNNModel(BaseModel):\n",
    "\n",
    "        def __init__(self, config=None, **kwargs):\n",
    "            config = config or {}\n",
    "            config.update(kwargs)\n",
    "\n",
    "            # Sequence parameters\n",
    "            self.window_size = self.safe_get(config, 'window_size', 30)\n",
    "            self.forecast_horizon = self.safe_get(config, 'forecast_horizon', 144)\n",
    "\n",
    "            # CNN architecture parameters\n",
    "            self.num_conv_layers = self.safe_get(config, 'num_conv_layers', 2)\n",
    "            self.num_filters = self.safe_get(config, 'num_filters', 64)\n",
    "            self.kernel_size = self.safe_get(config, 'kernel_size', 3)\n",
    "            self.dropout_rate = self.safe_get(config, 'dropout_rate', 0.3)\n",
    "            self.activation = self.safe_get(config, 'activation', 'relu')\n",
    "            self.use_batch_norm = self.safe_get(config, 'use_batch_norm', True)\n",
    "            self.use_global_pooling = self.safe_get(config, 'use_global_pooling', True)\n",
    "            self.version = self.safe_get(config, 'version', 'V1')  # support multiple architectures\n",
    "\n",
    "            # Training parameters\n",
    "            self.optimizer = self.safe_get(config, 'optimizer', 'adam')\n",
    "            self.loss = self.safe_get(config, 'loss', 'binary_crossentropy')\n",
    "            self.batch_size = self.safe_get(config, 'batch_size', 32)\n",
    "            self.epochs = self.safe_get(config, 'epochs', 20)\n",
    "            self.label_weights = self.safe_get(config, 'label_weights', None)\n",
    "\n",
    "            # Runtime config\n",
    "            self.verbose = self.safe_get(config, 'verbose', True)\n",
    "            self.model = None\n",
    "            self.history_buffer = []\n",
    "\n",
    "        def build(self, features, labels):\n",
    "            X_seq, y_seq = self.generate_sequence(features, labels)\n",
    "            input_shape = X_seq.shape[1:]\n",
    "            num_labels = y_seq.shape[1]\n",
    "            if self.version == 'V1':\n",
    "                self.V1(input_shape, num_labels)\n",
    "            elif self.version == 'V2':\n",
    "                print(\"V2 is trainnig\")\n",
    "                self.V2(input_shape, num_labels)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported CNN version: {self.version}\")\n",
    "            self.model.fit(X_seq, y_seq, batch_size=self.batch_size, epochs=self.epochs, verbose=self.verbose)\n",
    "\n",
    "        def V1(self, input_shape, num_labels):\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = inputs\n",
    "\n",
    "            for _ in range(self.num_conv_layers):\n",
    "                x = Conv1D(filters=self.num_filters,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           activation=self.activation,\n",
    "                           padding='same')(x)\n",
    "                if self.use_batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "\n",
    "            if self.use_global_pooling:\n",
    "                x = GlobalAveragePooling1D()(x)\n",
    "            else:\n",
    "                x = Flatten()(x)\n",
    "\n",
    "            outputs = Dense(num_labels, activation='sigmoid')(x)\n",
    "\n",
    "            model = Model(inputs, outputs)\n",
    "            model.compile(\n",
    "                optimizer=self.optimizer,\n",
    "                loss=self.loss,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            self.model = model\n",
    "\n",
    "\n",
    "        def V2(self, input_shape, num_labels):\n",
    "            \"\"\"\n",
    "            Builds a CNN model for sequence data with shape (window_size, num_features).\n",
    "            Expected input shape: (30, 37)\n",
    "            \"\"\"\n",
    "            inputs = Input(shape=input_shape)\n",
    "            x = inputs\n",
    "        \n",
    "            for _ in range(self.num_conv_layers):\n",
    "                x = Conv1D(filters=self.num_filters,\n",
    "                           kernel_size=self.kernel_size,\n",
    "                           activation=self.activation,\n",
    "                           padding='same')(x)\n",
    "                if self.use_batch_norm:\n",
    "                    x = BatchNormalization()(x)\n",
    "                x = Dropout(self.dropout_rate)(x)\n",
    "        \n",
    "            if self.use_global_pooling:\n",
    "                x = GlobalAveragePooling1D()(x)\n",
    "            else:\n",
    "                x = Flatten()(x)\n",
    "        \n",
    "            outputs = Dense(num_labels, activation='sigmoid')(x)\n",
    "        \n",
    "            model = Model(inputs, outputs)\n",
    "            model.compile(\n",
    "                optimizer=self.optimizer,\n",
    "                loss=self.loss,\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "        \n",
    "            self.model = model\n",
    "\n",
    "    class Evaluating:\n",
    "        \n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def evaluate_multilabel_metrics(self, y_true, y_pred, threshold=0.5):\n",
    "            y_pred_binary = (y_pred > threshold).astype(int)\n",
    "            label_names = None\n",
    "            if hasattr(y_true, 'columns'):\n",
    "                label_names = y_true.columns\n",
    "                y_true = y_true.to_numpy()\n",
    "                y_pred_binary = pd.DataFrame(y_pred_binary, columns=label_names).to_numpy()\n",
    "\n",
    "            print(\"\\nðŸ“Š Evaluation Metrics Summary\")\n",
    "            print(\"=\" * 40)\n",
    "\n",
    "            print(f\"âœ… Exact Match Accuracy : {accuracy_score(y_true, y_pred_binary):.4f}\\n\")\n",
    "\n",
    "            print(\"ðŸ”¹ Micro-Averaged Scores\")\n",
    "            print(f\"   Precision            : {precision_score(y_true, y_pred_binary, average='micro', zero_division=0):.4f}\")\n",
    "            print(f\"   Recall               : {recall_score(y_true, y_pred_binary, average='micro', zero_division=0):.4f}\")\n",
    "            print(f\"   F1 Score             : {f1_score(y_true, y_pred_binary, average='micro', zero_division=0):.4f}\\n\")\n",
    "\n",
    "            print(\"ðŸ”¹ Macro-Averaged Scores\")\n",
    "            print(f\"   Precision            : {precision_score(y_true, y_pred_binary, average='macro', zero_division=0):.4f}\")\n",
    "            print(f\"   Recall               : {recall_score(y_true, y_pred_binary, average='macro', zero_division=0):.4f}\")\n",
    "            print(f\"   F1 Score             : {f1_score(y_true, y_pred_binary, average='macro', zero_division=0):.4f}\")\n",
    "\n",
    "            print(\"\\nðŸ“‹ Full Classification Report\")\n",
    "            print(\"-\" * 40)\n",
    "            print(classification_report(y_true, y_pred_binary, target_names=label_names,zero_division=0))\n",
    "\n",
    "        def remove_low_metric_rows(self, df, columns, threshold=2):\n",
    "            \"\"\"\n",
    "            Remove rows where the number of zero values in the specified columns\n",
    "            is greater than or equal to the threshold.\n",
    "        \n",
    "            Parameters:\n",
    "                df (pd.DataFrame): Input DataFrame.\n",
    "                columns (list): List of columns to check for zero values.\n",
    "                threshold (int): Minimum number of zeros to trigger removal.\n",
    "        \n",
    "            Returns:\n",
    "                clean_df (pd.DataFrame): DataFrame with low-performing rows removed.\n",
    "            \"\"\"\n",
    "            # Only keep columns that actually exist\n",
    "            valid_columns = [col for col in columns if col in df.columns]\n",
    "        \n",
    "            if not valid_columns:\n",
    "                raise ValueError(\"None of the specified columns exist in the DataFrame.\")\n",
    "        \n",
    "            zero_count = (df[valid_columns] == 0).sum(axis=1)\n",
    "        \n",
    "            mask = zero_count < threshold\n",
    "            clean_df = df[mask].reset_index(drop=True)\n",
    "        \n",
    "            return clean_df\n",
    "\n",
    "        def select_best_models(self, df, filter_by: dict = None,group_by: list = None,sort_by: list = ['macro_f1', 'micro_f1','hamming_score'],sort_ascending: list = [True, True, True],top_n: int = 1):\n",
    "            \"\"\"\n",
    "            Select best-performing models based on filters, groupings, and sort criteria.\n",
    "\n",
    "            Parameters:\n",
    "                df (pd.DataFrame): DataFrame with model evaluation results.\n",
    "                filter_by (dict): Optional. Column-value filters, e.g., {'forecast_horizon': [90, 144]}.\n",
    "                group_by (list): Optional. Columns to group by, e.g., ['version', 'forecast_horizon'].\n",
    "                sort_by (list): Columns to sort by within each group. Defaults to F1 scores.\n",
    "                sort_ascending (list): Sort direction per column in sort_by. If None, defaults to all False.\n",
    "                top_n (int): Number of top models to return per group.\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Best model(s) per group.\n",
    "            \"\"\"\n",
    "\n",
    "            # Fix decimal formatting\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':\n",
    "                    df[col] = df[col].str.replace(',', '.', regex=False)\n",
    "\n",
    "            # Convert numeric columns\n",
    "            numeric_cols = [\n",
    "                \"forecast_horizon\", \"macro_f1\", \"micro_f1\", \"macro_precision\",\n",
    "                \"micro_precision\", \"macro_recall\", \"micro_recall\", \"window_size\",\n",
    "                \"num_filters\", \"kernel_size\", \"dropout_rate\", \"batch_size\", \"epochs\", \"hamming_score\"\n",
    "            ]\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "            # Apply filters\n",
    "            if filter_by:\n",
    "                for key, value in filter_by.items():\n",
    "                    if isinstance(value, list):\n",
    "                        df = df[df[key].isin(value)]\n",
    "                    else:\n",
    "                        df = df[df[key] == value]\n",
    "\n",
    "            # Handle sort direction\n",
    "            if sort_ascending is None:\n",
    "                sort_ascending = [False] * len(sort_by)\n",
    "\n",
    "            # Sort and select best per group\n",
    "            df_sorted = df.sort_values(by=sort_by, ascending=sort_ascending)\n",
    "            if group_by:\n",
    "                best_df = df_sorted.groupby(group_by).head(top_n).reset_index(drop=True)\n",
    "            else:\n",
    "                best_df = df_sorted.head(top_n).reset_index(drop=True)\n",
    "\n",
    "            return best_df\n",
    "\n",
    "    \n",
    "    class Ploting:\n",
    "        \n",
    "        def __init__(self):\n",
    "            pass\n",
    "        \n",
    "        def plot_confusion_matrices(self, y_true, y_pred, labels=None, figsize=(20, 8)):\n",
    "            \"\"\"\n",
    "            Plots 12 confusion matrices (2 rows Ã— 6 columns) for each machine's tool change and quality check labels.\n",
    "\n",
    "            Args:\n",
    "                y_true: Ground truth (DataFrame or array)\n",
    "                y_pred: Predicted binary labels\n",
    "                labels: Optional list of label names (in order)\n",
    "                figsize: Size of the entire figure\n",
    "            \"\"\"\n",
    "            if hasattr(y_true, 'columns'):\n",
    "                label_names = y_true.columns.tolist()\n",
    "            elif labels is not None:\n",
    "                label_names = labels\n",
    "            else:\n",
    "                label_names = [f\"Label {i}\" for i in range(y_true.shape[1])]\n",
    "\n",
    "            n_labels = min(len(label_names), y_true.shape[1])\n",
    "            y_true = np.array(y_true)\n",
    "            y_pred = np.array(y_pred)\n",
    "\n",
    "            fig, axes = plt.subplots(2, 6, figsize=figsize)\n",
    "            fig.suptitle(\"Confusion Matrices for Tool Change (Top) and Quality Check (Bottom)\", fontsize=16)\n",
    "\n",
    "            for i in range(n_labels):\n",
    "                label = label_names[i]\n",
    "                true_col = y_true[:, i]\n",
    "                pred_col = y_pred[:, i]\n",
    "\n",
    "                row = 0 if \"tool\" in label.lower() else 1\n",
    "                col = i // 2  # assuming label order is [MC1_tool, MC1_quality, MC2_tool, MC2_quality, ...]\n",
    "\n",
    "                if true_col.sum() == 0 and pred_col.sum() == 0:\n",
    "                    axes[row, col].axis(\"off\")\n",
    "                    axes[row, col].text(0.5, 0.5, f\"{label}\\n(no data)\", ha='center', va='center')\n",
    "                    continue\n",
    "                \n",
    "                cm = confusion_matrix(true_col, pred_col, labels=[0, 1])\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "                disp.plot(ax=axes[row, col], values_format='d', colorbar=False)\n",
    "                axes[row, col].set_title(label, fontsize=10)\n",
    "                axes[row, col].grid(False)\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        def plot_training_history(self, history):\n",
    "            if not hasattr(self, 'history'):\n",
    "                print(\"No training history found.\")\n",
    "                return\n",
    "            plt.figure(figsize=(12, 4))\n",
    "\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history['loss'], label='loss')\n",
    "            if 'val_loss' in history:\n",
    "                plt.plot(history['val_loss'], label='val_loss')\n",
    "            plt.title(\"Loss\")\n",
    "            plt.legend()\n",
    "\n",
    "            if self.metrics:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                metric = self.metrics[0]\n",
    "                plt.plot(history[metric], label=metric)\n",
    "                if f'val_{metric}' in history:\n",
    "                    plt.plot(history[f'val_{metric}'], label=f'val_{metric}')\n",
    "                plt.title(metric.capitalize())\n",
    "                plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        def cl_test_result_model(self, results, model, exclude_keys=None):\n",
    "            \"\"\"\n",
    "            Plots multiple metrics over runs using Seaborn for better visuals.\n",
    "        \n",
    "            Parameters:\n",
    "                results (list of dict): List of dictionaries with metric names as keys and float values.\n",
    "                model (str): Model name (e.g., 'TCN', 'LSTM') for plot titles and filenames.\n",
    "                exclude_keys (list of str): Optional. Additional keys to exclude from plotting.\n",
    "            \"\"\"\n",
    "            # Always exclude these\n",
    "            default_exclude = ['train_duration', 'predict_duration','run']\n",
    "            exclude_keys = default_exclude + (exclude_keys or [])\n",
    "        \n",
    "            df = pd.DataFrame(results)\n",
    "        \n",
    "            # Drop excluded columns\n",
    "            df = df.drop(columns=[k for k in exclude_keys if k in df.columns], errors='ignore')\n",
    "        \n",
    "            # Melt for Seaborn\n",
    "            df_melted = df.reset_index().melt(id_vars='index', var_name='Metric', value_name='Value')\n",
    "        \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.lineplot(data=df_melted, x='index', y='Value', hue='Metric', marker='o')\n",
    "            plt.xlabel('Run Index')\n",
    "            plt.ylabel('Metric Value')\n",
    "            plt.title(f'{model} Model Performance Trends During Continuous Learning')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/{model}performContinuousLearning.pdf')\n",
    "            plt.show()\n",
    "\n",
    "        def plot_binary_prediction_distribution(self, predictions, model_names=None):\n",
    "            \"\"\"\n",
    "            Plots side-by-side bar charts showing binary prediction counts (0s and 1s) for each model.\n",
    "\n",
    "            Parameters:\n",
    "                predictions (list of np.ndarray): List of binary prediction arrays (values: 0 or 1)\n",
    "                model_names (list of str): Names corresponding to each model\n",
    "            \"\"\"\n",
    "            if model_names is None:\n",
    "                model_names = [f\"Model {i+1}\" for i in range(len(predictions))]\n",
    "\n",
    "            counts = []\n",
    "            for pred in predictions:\n",
    "                flat = np.round(np.array(pred).flatten())\n",
    "                zeros = np.sum(flat == 0)\n",
    "                ones = np.sum(flat == 1)\n",
    "                counts.append([zeros, ones])\n",
    "\n",
    "            counts = np.array(counts)\n",
    "            x = np.arange(len(model_names))\n",
    "            width = 0.35\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8, 5))\n",
    "            ax.bar(x - width/2, counts[:, 0], width, label='Predicted 0')\n",
    "            ax.bar(x + width/2, counts[:, 1], width, label='Predicted 1')\n",
    "\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(model_names)\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            ax.set_title(\"Binary Prediction Distribution by Model\")\n",
    "            ax.legend()\n",
    "            plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "            plt.tight_layout()\n",
    "            fig.savefig(f'plots/models_prediction_distribution.pdf')\n",
    "            plt.show()\n",
    "\n",
    "        def plot_hyperparameter_distribution(self, df, model):\n",
    "            \"\"\"\n",
    "            Plots a stacked bar chart of hyperparameter value distributions,\n",
    "            and prints only the hyperparameters with more than one unique value.\n",
    "            \"\"\"\n",
    "            df = df.copy()\n",
    "\n",
    "            # Convert lists/arrays to strings so they can be counted\n",
    "            for col in df.columns:\n",
    "                if df[col].apply(lambda x: isinstance(x, (list, tuple)) or hasattr(x, '__array__')).any():\n",
    "                    df[col] = df[col].apply(lambda x: str(x))\n",
    "\n",
    "            # Filter to columns with multiple values\n",
    "            variable_columns = [col for col in df.columns if df[col].nunique() > 1]\n",
    "\n",
    "            if not variable_columns:\n",
    "                print(\"â„¹ï¸ All hyperparameters have a single unique value â€” nothing to plot.\")\n",
    "                return\n",
    "\n",
    "            print(f\"ðŸ“Š Hyperparameters with >1 value ({len(variable_columns)}):\")\n",
    "            for col in variable_columns:\n",
    "                unique_vals = df[col].unique()\n",
    "                print(f\" - {col}: {list(unique_vals)}\")\n",
    "\n",
    "            # Plot\n",
    "            palette = sns.color_palette(\"muted\", 10)\n",
    "            fig_width = max(1.8 * len(variable_columns), 12)\n",
    "            fig, ax = plt.subplots(figsize=(fig_width, 6), constrained_layout=True)\n",
    "\n",
    "            bar_width = 0.8\n",
    "            x_positions = range(len(variable_columns))\n",
    "\n",
    "            for idx, col in enumerate(variable_columns):\n",
    "                counts = df[col].value_counts()\n",
    "                bottom = 0\n",
    "                for i, (val, count) in enumerate(counts.items()):\n",
    "                    ax.bar(idx, count, bottom=bottom, color=palette[i % len(palette)], width=bar_width)\n",
    "                    ax.text(idx, bottom + count / 2, f'{val} ({count})',\n",
    "                            ha='center', va='center', fontsize=10, color='black', fontweight='bold')\n",
    "                    bottom += count\n",
    "\n",
    "            ax.set_xticks(x_positions)\n",
    "            ax.set_xticklabels(variable_columns, rotation=45, ha='right')\n",
    "            ax.set_ylabel(\"Count\")\n",
    "            ax.set_title(f\"Distribution of Hyperparameter Values for {model}\")\n",
    "            fig.savefig(f'plots/{model}paramdistro.pdf')\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "        def calculate_metrics(models, predictions, y_true, threshold=0.5, show_plot=True):\n",
    "            \"\"\"\n",
    "            Calculate and compare model performance metrics.\n",
    "\n",
    "            Parameters:\n",
    "                models (list of str): Names of models\n",
    "                predictions (list of np.ndarray): Predictions for each model (probabilities or binary)\n",
    "                y_true (np.ndarray): Ground truth labels\n",
    "                threshold (float): Threshold to binarize predictions if needed\n",
    "                show_plot (bool): Whether to display a bar chart of the results\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: DataFrame with metrics for each model\n",
    "\n",
    "            Usage:\n",
    "                model_names = ['TCN', 'LSTM', 'CNN']\n",
    "                preds = [tcn_y_pred, lsmt_y_pred, cnn_y_pred]\n",
    "\n",
    "                df_metrics = calculate_metrics(model_names, preds, y_true)\n",
    "            \"\"\"\n",
    "            results = []\n",
    "\n",
    "            for name, pred in zip(models, predictions):\n",
    "                pred_bin = (np.array(pred) > threshold).astype(int)\n",
    "\n",
    "                metrics = {\n",
    "                    \"Model\": name,\n",
    "                    \"Macro F1\": f1_score(y_true, pred_bin, average='macro', zero_division=0),\n",
    "                    \"Micro F1\": f1_score(y_true, pred_bin, average='micro', zero_division=0),\n",
    "                    \"Macro Precision\": precision_score(y_true, pred_bin, average='macro', zero_division=0),\n",
    "                    \"Micro Precision\": precision_score(y_true, pred_bin, average='micro', zero_division=0),\n",
    "                    \"Macro Recall\": recall_score(y_true, pred_bin, average='macro', zero_division=0),\n",
    "                    \"Micro Recall\": recall_score(y_true, pred_bin, average='micro', zero_division=0),\n",
    "                    \"Hamming Score\": 1 - hamming_loss(y_true, pred_bin)\n",
    "                }\n",
    "                results.append(metrics)\n",
    "\n",
    "            df = pd.DataFrame(results)\n",
    "\n",
    "            if show_plot:\n",
    "                df_melted = df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.barplot(data=df_melted, x=\"Metric\", y=\"Score\", hue=\"Model\")\n",
    "                plt.title(\"Model Performance Comparison\")\n",
    "                plt.xticks(rotation=30, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'plots/metricsbarchart.pdf')\n",
    "                plt.show()\n",
    "\n",
    "            return df\n",
    "        \n",
    "\n",
    "        def calculate_per_label_f1(self, models, predictions, y_true, threshold=0.5):\n",
    "            \"\"\"\n",
    "            Plots per-label macro F1 scores for multiple models.\n",
    "\n",
    "            Parameters:\n",
    "                models (list of str): List of model names\n",
    "                predictions (list of np.ndarray): List of prediction arrays, shape (n_samples, n_labels)\n",
    "                y_true (np.ndarray): Ground truth array, shape (n_samples, n_labels)\n",
    "                threshold (float): Binarization threshold\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Per-label F1 scores for each model\n",
    "\n",
    "            Usage: \n",
    "                model_names = ['TCN', 'LSTM', 'CNN']\n",
    "                preds = [tcn_y_pred, lsmt_y_pred, cnn_y_pred]\n",
    "                \n",
    "                df_label_f1 = model.plot.calculate_per_label_f1(model_names, preds, y_true)\n",
    "            \"\"\"\n",
    "            label_names = [\n",
    "                \"MC1_tool\", \"MC1_quality\", \"MC2_tool\", \"MC2_quality\",\n",
    "                \"MC3_tool\", \"MC3_quality\", \"MC4_tool\", \"MC4_quality\",\n",
    "                \"MC5_tool\", \"MC5_quality\", \"MC6_tool\", \"MC6_quality\"\n",
    "            ]\n",
    "            results = []\n",
    "\n",
    "            for name, pred in zip(models, predictions):\n",
    "                pred_bin = (np.array(pred) > threshold).astype(int)\n",
    "                for i, label in enumerate(label_names):\n",
    "                    score = f1_score(y_true[:, i], pred_bin[:, i], zero_division=0)\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Label': label,\n",
    "                        'F1 Score': score\n",
    "                    })\n",
    "\n",
    "            df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(14, 6))\n",
    "            sns.barplot(data=df, x='Label', y='F1 Score', hue='Model')\n",
    "            plt.title(\"Per-Label F1 Score Comparison Across Models\")\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/f1scoreovemachineandmodel.pdf')\n",
    "            plt.show()\n",
    "\n",
    "            return df\n",
    "\n",
    "\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "        self.arch = config.get(\"arch\", \"tcn\").lower()\n",
    "\n",
    "        self.data_preparation = self.DataPreparation()\n",
    "        self.plot = self.Ploting()\n",
    "        self.evaluating = self.Evaluating()\n",
    "\n",
    "        if self.arch == \"tcn\":\n",
    "            self.model = self.TCNModel(config=config)\n",
    "        elif self.arch == \"lstm\":\n",
    "            self.model = self.LSTMModel(config=config)\n",
    "        elif self.arch == \"cnn\":\n",
    "            self.model = self.CNNModel(config=config)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported architecture: {arch}\")\n",
    "\n",
    "    def fit(self, data):\n",
    "        features, labels = self.data_preparation.get_train_data(data)\n",
    "        self.model.build(features, labels)\n",
    "\n",
    "    def fit_prepared(self,data):\n",
    "            self.model.build(data['X'], data['y'])\n",
    "      \n",
    "\n",
    "    def predict(self, x):\n",
    "        #x = self.data_preparation.get_predict_row(x)\n",
    "        return self.model.model.predict(x)\n",
    "    \n",
    "\n",
    "    def train_validate(self, df, train_size=0.7, validate_size=0.15):\n",
    "        train, validate, test, label_names = self.data_preparation.split_data(df, train_size, validate_size)\n",
    "    \n",
    "        self.model.build(train['X'], train['y'])\n",
    "    \n",
    "        X_val_seq, y_val_seq = self.model.generate_sequence(validate['X'], validate['y'])\n",
    "        val_preds = self.model.predict(X_val_seq)\n",
    "    \n",
    "        X_test_seq, y_test_seq = self.model.generate_sequence(test['X'], test['y'])\n",
    "        test_preds = self.model.predict(X_test_seq)\n",
    "    \n",
    "        return {\n",
    "            'validate': {'y_true': y_val_seq, 'y_pred': val_preds, 'label_names': label_names},\n",
    "            'test': {'y_true': y_test_seq, 'y_pred': test_preds, 'label_names': label_names}\n",
    "        }\n",
    "    \n",
    "    def train_validate_prepared(self, train, validate, label_names):\n",
    "       \n",
    "        self.model.build(train['X'], train['y'])\n",
    "    \n",
    "        X_val_seq, y_val_seq = self.model.generate_sequence(validate['X'], validate['y'])\n",
    "        val_preds = self.model.model.predict(X_val_seq)\n",
    "    \n",
    "        return {\n",
    "            'validate': {'y_true': y_val_seq, 'y_pred': val_preds, 'label_names': label_names},\n",
    "        }\n",
    "\n",
    "    def confusion_matrices(self,y_true, y_pred, label_names):\n",
    "        self.evaluating.evaluate_multilabel_metrics(y_true=y_true,y_pred=y_pred)\n",
    "        self.plot.plot_confusion_matrices(y_true=y_true,y_pred=y_pred,labels=label_names)\n",
    "\n",
    "\n",
    "    def grid_search(self,  train, validate, label_names, param_grid, threshold=0.5, save_path=None):\n",
    "        \"\"\"\n",
    "        Runs grid search on TCN model parameters using pre-split training and validation sets.\n",
    "\n",
    "        Args:\n",
    "            train (dict): Dictionary containing training data with keys 'X' and 'y'\n",
    "            validate (dict): Dictionary containing validation data with keys 'X' and 'y'\n",
    "            label_names (list): List of label column names\n",
    "            param_grid (dict): Dict of parameters to test (e.g. {'window_size': [30, 60], 'num_filters': [32, 64]})\n",
    "            threshold (float): Threshold for binarizing predictions\n",
    "            save_path (str): Optional path to save results as CSV\n",
    "\n",
    "        Returns:\n",
    "            List[dict]: A list of result dictionaries for each parameter combination, including macro/micro metrics.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        param_combos = list(itertools.product(*param_grid.values()))\n",
    "        param_names = list(param_grid.keys())\n",
    "        total = 192\n",
    "        current = 1\n",
    "\n",
    "        for combo in param_combos:\n",
    "            config = dict(zip(param_names, combo))\n",
    "            print(f\"ðŸ”model {current}/{total} Testing config: {config}\")\n",
    "            current += 1\n",
    "\n",
    "            result = self._evaluate_single_config(config, train, validate, label_names, threshold)\n",
    "            results.append(result)\n",
    "\n",
    "        if save_path:\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(save_path, index=False)\n",
    "            print(f\"ðŸ“ Results saved to: {save_path}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    def _evaluate_single_config(self, config, train, validate, label_names, threshold):\n",
    "        try:\n",
    "            model = ModelFactory(config)\n",
    "            res = model.train_validate_prepared(train, validate, label_names)\n",
    "            y_true = res['validate']['y_true']\n",
    "            y_pred = (res['validate']['y_pred'] > threshold).astype(int)\n",
    "            if self.arch == 'lstm':\n",
    "                y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "            metrics = {\n",
    "                'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'hamming_score': 1 - hamming_loss(y_true, y_pred),\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "               \n",
    "            }\n",
    "\n",
    "            # Flatten config into top-level keys\n",
    "            combined_result = {**config, **metrics}\n",
    "            return combined_result\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed for config {config}: {e}\")\n",
    "            return {**config, 'error': str(e)}\n",
    "        \n",
    "    def model_perform(self, path, top_n: int = 1):\n",
    "        columns_to_check = [\n",
    "        'accuracy',\n",
    "        'micro_f1',\n",
    "        'macro_f1',\n",
    "        'macro_precision',\n",
    "        'micro_precision',\n",
    "        'macro_recall',\n",
    "        'micro_recall',\n",
    "        'hamming_score',\n",
    "        'label_weights',\n",
    "        ]\n",
    "        df = pd.read_csv(path)   \n",
    "        res = self.evaluating.remove_low_metric_rows(df,columns_to_check)\n",
    "        config = self.evaluating.select_best_models(res, top_n=top_n)\n",
    "        configs = config.drop(columns_to_check, axis=1, errors='ignore')\n",
    "        if top_n > 1:\n",
    "            self.plot.plot_hyperparameter_distribution(configs, self.arch)\n",
    "\n",
    "        return config, configs\n",
    "\n",
    "    def run_continual_training_with_labels(self,cl_train, cl_test, cl_list):\n",
    "        \"\"\"\n",
    "        Run continual learning evaluation and return both overall and per-label results.\n",
    "\n",
    "        Parameters:\n",
    "            model_factory: Instance of ModelFactory (e.g., cl_lstm)\n",
    "            cl_test: dict with 'X' and 'y' for initial evaluation and update\n",
    "            cl_list: list of (train, test) batches\n",
    "            label_names: list of str, column names for labels\n",
    "            model_name: str, used for labeling plots and output\n",
    "\n",
    "        Returns:\n",
    "            overall_results: list of dicts, metrics for each run\n",
    "            label_results: list of dicts, per-label F1/Precision/Recall/Accuracy per run\n",
    "        \"\"\"\n",
    "        overall_results = []\n",
    "        label_results = []\n",
    "\n",
    "        def evaluate(run_idx, y_true, y_pred, label_names):\n",
    "            y_pred_bin = (y_pred > 0.5).astype(int)\n",
    "            metrics = {\n",
    "                'run': run_idx,\n",
    "                'macro_f1': f1_score(y_true, y_pred_bin, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred_bin, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred_bin, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred_bin, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred_bin, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred_bin, average='micro', zero_division=0),\n",
    "                'accuracy': accuracy_score(y_true, y_pred_bin)\n",
    "            }\n",
    "\n",
    "            # Add learning rate\n",
    "            try:\n",
    "                optimizer = self.model.model.optimizer\n",
    "                if hasattr(optimizer, '_decayed_lr'):\n",
    "                    lr = optimizer._decayed_lr(tf.float32).numpy()\n",
    "                else:\n",
    "                    lr = optimizer.learning_rate.numpy()\n",
    "                metrics['learning_rate'] = lr\n",
    "            except Exception as e:\n",
    "                metrics['learning_rate'] = None  # fallback if something goes wrong\n",
    "\n",
    "            # Per-label breakdown\n",
    "            for i, label in enumerate(label_names):\n",
    "                label_metrics = {\n",
    "                    'run': run_idx,\n",
    "                    'label': label,\n",
    "                    'f1': f1_score(y_true[:, i], y_pred_bin[:, i], zero_division=0),\n",
    "                    'precision': precision_score(y_true[:, i], y_pred_bin[:, i], zero_division=0),\n",
    "                    'recall': recall_score(y_true[:, i], y_pred_bin[:, i], zero_division=0),\n",
    "                    'accuracy': accuracy_score(y_true[:, i], y_pred_bin[:, i])\n",
    "                }\n",
    "                label_results.append(label_metrics)\n",
    "\n",
    "            return metrics\n",
    "\n",
    "        # Initial evaluation before any training\n",
    "        self.fit_prepared(cl_train)\n",
    "        X_val_seq, y_true = self.model.generate_sequence(cl_test['X'], cl_test['y'])\n",
    "        y_pred = self.model.model.predict(X_val_seq)\n",
    "        label_names = [col for col in cl_list[0][2] if col != \"Timestamp\"]\n",
    "        overall_results.append(evaluate(-1, y_true, y_pred,label_names))\n",
    "        self.fit_prepared(cl_test)\n",
    "\n",
    "        # Run over batches\n",
    "        for i, (train, test,_) in enumerate(cl_list):\n",
    "            self.fit_prepared(train)\n",
    "            X_val_seq, y_true = self.model.generate_sequence(test['X'], test['y'])\n",
    "            y_pred = self.model.model.predict(X_val_seq)\n",
    "\n",
    "\n",
    "            metrics = evaluate(i, y_true, y_pred,label_names)\n",
    "\n",
    "            overall_results.append(metrics)\n",
    "            self.fit_prepared(test)\n",
    "\n",
    "        self.plot.cl_test_result_model(overall_results, self.arch)\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "        overall_df = pd.DataFrame(overall_results)\n",
    "        label_df = pd.DataFrame(label_results)\n",
    "\n",
    "        overall_df.to_csv(f\"results/{ self.arch}_overall_results.csv\", index=False)\n",
    "        label_df.to_csv(f\"results/{ self.arch}_label_results.csv\", index=False)\n",
    "\n",
    "        return overall_results, label_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb')as f:\n",
    "        return pickle.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = [\n",
    "    \"MC1_tool\", \"MC1_quality\", \"MC2_tool\", \"MC2_quality\",\n",
    "    \"MC3_tool\", \"MC3_quality\", \"MC4_tool\", \"MC4_quality\",\n",
    "    \"MC5_tool\", \"MC5_quality\", \"MC6_tool\", \"MC6_quality\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../simulations/logs/year/linelog.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tools \u001b[38;5;241m=\u001b[39m ModelFactory()\n\u001b[1;32m----> 2\u001b[0m year_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../simulations/logs/year/linelog.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../simulations/logs/year/linelog.csv'"
     ]
    }
   ],
   "source": [
    "tools = ModelFactory()\n",
    "year_data = pd.read_csv('../simulations/logs/year/line_log.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_train = load_pickle(\"data/gs_train.pkl\")\n",
    "gs_validate = load_pickle(\"data/gs_validate.pkl\")\n",
    "gs_test = load_pickle(\"data/gs_test.pkl\")\n",
    "labels = load_pickle(\"data/labels.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = ModelFactory()\n",
    "label = pd.DataFrame(gs_train['y'])\n",
    "label.drop(columns=['Timestamp'], inplace=True)\n",
    "lw = base.data_preparation.compute_label_weights(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_base_config = {\n",
    "    'arch':'tcn',\n",
    "    'window_size': 30, \n",
    "    'forecast_horizon': 144, \n",
    "    'num_filters': 32, \n",
    "    'kernel_size': 2, \n",
    "    'dropout_rate': 0.3, \n",
    "    'activation': 'relu', \n",
    "    'dilations': '[1. 2. 4. 8.]'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_param_grid = {\n",
    "    \n",
    "    'window_size': [30, 60],                      \n",
    "    'forecast_horizon': [90, 144],       \n",
    "    'num_filters': [32, 64],                     \n",
    "    'kernel_size': [2, 3],                        \n",
    "    'dropout_rate': [0.1, 0.3],                   \n",
    "    'activation': ['relu', 'tanh', 'elu'],        \n",
    "    'dilations': [[1,2,4,8],[1,2,4,8,16,32]],\n",
    "    'label_weights': [lw],\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn = ModelFactory(tcn_base_config)\n",
    "\n",
    "\n",
    "save_dir = \"grid_search_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)  \n",
    "\n",
    "save_path = os.path.join(save_dir, \"gs_tcn_res_test.csv\")\n",
    "results = tcn.grid_search(gs_train, gs_validate, labels, tcn_base_config ,save_path=f'grid_search_results/gs_tcn_res_test.csv')\n",
    "\n",
    "# Sort by best macro F1\n",
    "sorted_results = sorted(results, key=lambda x: x.get('macro_f1', 0), reverse=True)\n",
    "\n",
    "def safe_fmt(x):\n",
    "    return f\"{x:.4f}\" if isinstance(x, (float, int)) else str(x)\n",
    "\n",
    "for r in sorted_results[:5]:  # Top 5 configs\n",
    "    print(f\"   Macro F1: {safe_fmt(r.get('macro_f1', '-'))}, \"\n",
    "          f\"Precision: {safe_fmt(r.get('macro_precision', '-'))}, \"\n",
    "          f\"Recall: {safe_fmt(r.get('macro_recall', '-'))}\")\n",
    "    print(f\"   Micro F1: {safe_fmt(r.get('micro_f1', '-'))}, \"\n",
    "          f\"Precision: {safe_fmt(r.get('micro_precision', '-'))}, \"\n",
    "          f\"Recall: {safe_fmt(r.get('micro_recall', '-'))}\")\n",
    "    print(f\"Hamming : {safe_fmt(r.get('hamming_score', '-'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn.model_perform('grid_search_results/gs_tcn_res_test.csv',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "best_conf = tcn.model_perform('grid_search_results/gs_tcn_res_test.csv',1)\n",
    "best_conf[0].drop(['macro_f1','micro_f1','macro_precision', 'micro_precision', \n",
    "              'macro_recall', 'micro_recall','hamming_score', 'accuracy'], \n",
    "             axis=1, inplace=True)\n",
    "best_conf[0]['arch'] = 'tcn'\n",
    "\n",
    "\n",
    "tcn_conf = best_conf[0].iloc[0].to_dict()\n",
    "print(tcn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn_compare = ModelFactory(tcn_conf)\n",
    "tcn_compare.fit_prepared(gs_train)\n",
    "with open('models/tcn.pkl','wb') as f:\n",
    "    pickle.dump(tcn_compare, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continual Learning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_tcn = ModelFactory(tcn_conf)\n",
    "cl_tcn.fit_prepared(cl_train)\n",
    "results = []\n",
    "X_val_seq, y_true = cl_tcn.model.generate_sequence(cl_test['X'], cl_test['y'])\n",
    "y_pred = cl_tcn.model.model.predict(X_val_seq)\n",
    "\n",
    "metrics = {\n",
    "                'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'hamming_score': 1 - hamming_loss(y_true, y_pred),\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "               \n",
    "            }\n",
    "results.append(metrics)\n",
    "cl_tcn.fit_prepared(cl_test)\n",
    "\n",
    "for batch in cl_list:\n",
    "    train = batch[0]\n",
    "    test = batch[1]\n",
    "\n",
    "    start_train = time.time()\n",
    "    cl_tcn.fit_prepared(train)\n",
    "    mark = time.time()\n",
    "\n",
    "    X_val_seq, y_true = cl_tcn.model.generate_sequence(test['X'], test['y'])\n",
    "    y_pred = cl_tcn.model.model.predict(X_val_seq)\n",
    "    end_predict = time.time() - mark\n",
    "    metrics = {\n",
    "                'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'hamming_score': 1 - hamming_loss(y_true, y_pred),\n",
    "                'train_duration':mark-start_train,\n",
    "                'predict_duration':end_predict\n",
    "               \n",
    "            }\n",
    "    results.append(metrics)\n",
    "    cl_tcn.fit_prepared(test)\n",
    "\n",
    "cl_tcn.plot.cl_test_result_model(results,'TCN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_tcn = None\n",
    "cl_tcn = ModelFactory(tcn_conf)\n",
    "cl_tcn.run_continual_training_with_labels(cl_train, cl_test, cl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_tcn = None\n",
    "cl_tcn = ModelFactory(tcn_conf)\n",
    "cl_tcn.run_continual_training_with_labels(cl_train,cl_test,cl_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn = ModelFactory(tcn_config)\n",
    "tcn.fit_prepared(gs_train)\n",
    "\n",
    "with open('models/tcn.pkl','wb') as f:\n",
    "    pickle.dump(tcn, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_base_config = {\n",
    "    'arch': 'cnn',\n",
    "    'window_size': 30,\n",
    "    'forecast_horizon': 1,\n",
    "    'num_conv_layers': 2,\n",
    "    'num_filters': 64,\n",
    "    'kernel_size': 3,\n",
    "    'dropout_rate': 0.3,\n",
    "    'activation': 'relu',\n",
    "    'use_batch_norm': True,\n",
    "    'use_global_pooling': True,\n",
    "    'version': 'V1',\n",
    "    'optimizer': 'adam',\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'batch_size': 32,\n",
    "    'epochs': 20,\n",
    "    'label_weights': None,\n",
    "    'verbose': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_param_grid = {\n",
    "    'arch': ['cnn'],\n",
    "    'window_size': [30, 60],\n",
    "\n",
    "    'num_conv_layers': [1, 2, 3],\n",
    "    'num_filters': [32, 64],\n",
    "    'kernel_size': [2, 3],\n",
    "    'dropout_rate': [0.1, 0.3],\n",
    "    'activation': ['relu', 'elu'],\n",
    "    'use_batch_norm': [True, False],\n",
    "    'use_global_pooling': [True], \n",
    "    'version': ['V1'],\n",
    "\n",
    "    'batch_size': [32],\n",
    "    'epochs': [10],  \n",
    "    'label_weights': [lw],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = ModelFactory(cnn_base_config)\n",
    "\n",
    "\n",
    "save_dir = \"grid_search_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)  \n",
    "\n",
    "save_path = os.path.join(save_dir, \"gs_cnn_res_test.csv\")\n",
    "results = cnn.grid_search(gs_train, gs_validate, labels, cnn_param_grid ,save_path=f'grid_search_results/gs_cnn_res_test.csv')\n",
    "\n",
    "sorted_results = sorted(results, key=lambda x: x.get('macro_f1', 0), reverse=True)\n",
    "\n",
    "def safe_fmt(x):\n",
    "    return f\"{x:.4f}\" if isinstance(x, (float, int)) else str(x)\n",
    "\n",
    "for r in sorted_results[:5]: \n",
    "    print(f\"   Macro F1: {safe_fmt(r.get('macro_f1', '-'))}, \"\n",
    "          f\"Precision: {safe_fmt(r.get('macro_precision', '-'))}, \"\n",
    "          f\"Recall: {safe_fmt(r.get('macro_recall', '-'))}\")\n",
    "    print(f\"   Micro F1: {safe_fmt(r.get('micro_f1', '-'))}, \"\n",
    "          f\"Precision: {safe_fmt(r.get('micro_precision', '-'))}, \"\n",
    "          f\"Recall: {safe_fmt(r.get('micro_recall', '-'))}\")\n",
    "    print(f\"Hamming : {safe_fmt(r.get('hamming_score', '-'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = cnn.model_perform('grid_search_results/gs_cnn_res_test.csv',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_conf = cnn.model_perform('grid_search_results/gs_cnn_res_test.csv',1)\n",
    "best_conf[0].drop(['macro_f1','micro_f1','macro_precision', 'micro_precision', \n",
    "              'macro_recall', 'micro_recall','hamming_score', 'accuracy'], \n",
    "             axis=1, inplace=True)\n",
    "best_conf[0]['arch'] = 'cnn'\n",
    "\n",
    "\n",
    "cnn_conf = best_conf[0].iloc[0].to_dict()\n",
    "print(cnn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_compare = ModelFactory(cnn_conf)\n",
    "cnn_compare.fit_prepared(gs_train)\n",
    "with open('models/cnn.pkl','wb') as f:\n",
    "    pickle.dump(cnn_compare, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continual Learning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_cnn = ModelFactory(cnn_conf)\n",
    "\n",
    "cl_cnn.fit_prepared(cl_train)\n",
    "\n",
    "results = []\n",
    "X_val_seq, y_true = cl_cnn.model.generate_sequence(cl_test['X'], cl_test['y'])\n",
    "y_pred = cl_cnn.model.model.predict(X_val_seq)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "metrics = {\n",
    "                'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'hamming_score': 1 - hamming_loss(y_true, y_pred),\n",
    "                \n",
    "            }\n",
    "\n",
    "results.append(metrics)\n",
    "cl_cnn.fit_prepared(cl_test)\n",
    "\n",
    "for batch in cl_list:\n",
    "    train = batch[0]\n",
    "    test = batch[1]\n",
    "\n",
    "    start_train = time.time()\n",
    "    cl_cnn.fit_prepared(train)\n",
    "    mark = time.time()\n",
    "\n",
    "    X_val_seq, y_true = cl_cnn.model.generate_sequence(test['X'], test['y'])\n",
    "    y_pred = cl_cnn.model.model.predict(X_val_seq)\n",
    "    end_predict = time.time() - mark\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    metrics = {\n",
    "                'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'hamming_score': 1 - hamming_loss(y_true, y_pred),\n",
    "                'train_duration':mark-start_train,\n",
    "                'predict_duration':end_predict\n",
    "               \n",
    "            }\n",
    "    results.append(metrics)\n",
    "    cl_cnn.fit_prepared(test)\n",
    "\n",
    "cl_cnn.plot.cl_test_result_model(results,'CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_cnn = None\n",
    "cl_cnn = ModelFactory(cnn_conf)\n",
    "cl_cnn.run_continual_training_with_labels(cl_train, cl_test, cl_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_base_config = {\n",
    "    \"arch\": \"lstm\",\n",
    "    \"window_size\": 30,\n",
    "    \"forecast_horizon\": 1,\n",
    "    \"num_units\": 64,\n",
    "    \"dropout_rate\": 0.3,\n",
    "    \"num_layers\": 1,\n",
    "    \"bidirectional\": True,\n",
    "    \"return_sequences\": False,\n",
    "    \"activation\": \"tanh\",  \n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 20,\n",
    "    \"label_weights\": None,\n",
    "    \"verbose\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_param_grid = {\n",
    "    'arch': ['lstm'],\n",
    "    'window_size': [30, 60],\n",
    "    'forecast_horizon': [1, 10],\n",
    "    'num_units': [32, 64, 128],\n",
    "    'dropout_rate': [0.2, 0.4],\n",
    "    'num_layers': [1, 2],\n",
    "    'bidirectional': [False, True],\n",
    "    'activation': ['tanh'],        \n",
    "    'optimizer': ['adam'],\n",
    "    'loss': ['binary_crossentropy'],\n",
    "    'batch_size': [32],\n",
    "    'epochs': [20]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = ModelFactory(lstm_base_config)\n",
    "\n",
    "\n",
    "save_dir = \"grid_search_results\"\n",
    "os.makedirs(save_dir, exist_ok=True)  \n",
    "\n",
    "save_path = os.path.join(save_dir, \"gs_lstm_res_test.csv\")\n",
    "results = lstm.grid_search(gs_train, gs_validate, labels, lstm_param_grid ,save_path=f'grid_search_results/gs_lstm_res_test.csv')\n",
    "\n",
    "# Sort by best macro F1\n",
    "sorted_results = sorted(results, key=lambda x: x.get('macro_f1', 0), reverse=True)\n",
    "\n",
    "def safe_fmt(x):\n",
    "    return f\"{x:.4f}\" if isinstance(x, (float, int)) else str(x)\n",
    "\n",
    "for r in sorted_results[:5]:  # Top 5 configs\n",
    "    print(f\"   Macro F1: {safe_fmt(r.get('macro_f1', '-'))}, \"\n",
    "          f\"Precision: {safe_fmt(r.get('macro_precision', '-'))}, \"\n",
    "          f\"Recall: {safe_fmt(r.get('macro_recall', '-'))}\")\n",
    "    print(f\"   Micro F1: {safe_fmt(r.get('micro_f1', '-'))}, \"\n",
    "          f\"Precision: {safe_fmt(r.get('micro_precision', '-'))}, \"\n",
    "          f\"Recall: {safe_fmt(r.get('micro_recall', '-'))}\")\n",
    "    print(f\"Hamming : {safe_fmt(r.get('hamming_score', '-'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = lstm.model_perform('grid_search_results/gs_lstm_res_test.csv',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_conf = lstm.model_perform('grid_search_results/gs_lstm_res_test.csv',1)\n",
    "best_conf[0].drop(['macro_f1','micro_f1','macro_precision', 'micro_precision', \n",
    "              'macro_recall', 'micro_recall','hamming_score', 'accuracy'], \n",
    "             axis=1, inplace=True)\n",
    "best_conf[0]['arch'] = 'lstm'\n",
    "\n",
    "\n",
    "lstm_conf = best_conf[0].iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_compare = ModelFactory(lstm_conf)\n",
    "lstm_compare.fit_prepared(gs_train)\n",
    "with open('models/lstm.pkl','wb') as f:\n",
    "    pickle.dump(lstm_compare, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continual Learning Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_lstm = ModelFactory(lstm_conf)\n",
    "cl_lstm.fit_prepared(cl_train)\n",
    "results = []\n",
    "X_val_seq, y_true = cl_lstm.model.generate_sequence(cl_test['X'], cl_test['y'])\n",
    "y_pred = cl_lstm.model.model.predict(X_val_seq)\n",
    "y_pred = (y_pred > 0.5).astype(int)\n",
    "metrics = {\n",
    "                'run': -1,\n",
    "                'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'hamming_score': 1 - hamming_loss(y_true, y_pred),\n",
    "                \n",
    "            }\n",
    "\n",
    "results.append(metrics)\n",
    "cl_lstm.fit_prepared(cl_test)\n",
    "\n",
    "\n",
    "for i, batch in enumerate(cl_list):\n",
    "    train = batch[0]\n",
    "    test = batch[1]\n",
    "\n",
    "    start_train = time.time()\n",
    "    cl_lstm.fit_prepared(train)\n",
    "    mark = time.time()\n",
    "\n",
    "    X_val_seq, y_true = cl_lstm.model.generate_sequence(test['X'], test['y'])\n",
    "    y_pred = cl_lstm.model.model.predict(X_val_seq)\n",
    "    end_predict = time.time() - mark\n",
    "    y_pred = (y_pred > 0.5).astype(int)\n",
    "    metrics = {\n",
    "                'run' : i,\n",
    "                'macro_f1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_f1': f1_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_precision': precision_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'macro_recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
    "                'micro_recall': recall_score(y_true, y_pred, average='micro', zero_division=0),\n",
    "                'hamming_score': 1 - hamming_loss(y_true, y_pred),\n",
    "                'train_duration':mark-start_train,\n",
    "                'predict_duration':end_predict\n",
    "               \n",
    "            }\n",
    "    results.append(metrics)\n",
    "    cl_lstm.fit_prepared(test)\n",
    "\n",
    "cl_lstm.plot.cl_test_result_model(results,'LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_conf = {'arch': 'cnn', 'window_size': 60, 'num_conv_layers': 2, 'num_filters': 32, 'kernel_size': 2, 'dropout_rate': 0.1, 'activation': 'relu', 'use_batch_norm': True, 'use_global_pooling': True, 'version': 'V1', 'batch_size': 32, 'epochs': 10, 'label_weights': lw}\n",
    "lstm_conf = {'arch': 'lstm','window_size': 30,'forecast_horizon': 144,'num_units': 32,'dropout_rate': 0.2,'num_layers': 2,'bidirectional': True,'activation': 'tanh','optimizer': 'adam','loss': 'binary_crossentropy','batch_size': 32,'epochs': 20}\n",
    "tcn_conf = {'arch':'tcn','window_size': 30, 'forecast_horizon': 144, 'num_filters': 32, 'kernel_size': 2, 'dropout_rate': 0.3, 'activation': 'elu', 'dilations': '[1. 2. 4. 8. 16. 32]','label_weights': lw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_lstm = None\n",
    "cl_lstm = ModelFactory(lstm_conf)\n",
    "cl_lstm.run_continual_training_with_labels(cl_train,cl_test,cl_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_seq, y_true = cnn.model.generate_sequence(gs_test['X'], gs_test['y'])\n",
    "tcn_y_pred = tcn.model.model.predict(X_val_seq)\n",
    "lsmt_y_pred = lstm.model.model.predict(X_val_seq)\n",
    "lsmt_y_pred = (lsmt_y_pred > 0.5).astype(int)\n",
    "cnn_y_pred = cnn.model.model.predict(X_val_seq)\n",
    "cnn_y_pred = (cnn_y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['TCN', 'LSTM', 'CNN']\n",
    "preds = [tcn_y_pred, lsmt_y_pred, cnn_y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = tcn.plot.calculate_metrics(model_names, preds, y_true)\n",
    "display(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label_f1 = tcn.plot.calculate_per_label_f1(model_names, preds, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcn.plot.plot_binary_prediction_distribution(preds, model_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_models_combined(df, metric_cols,path=None):\n",
    "    \"\"\"\n",
    "    Plot selected metrics on the same figure for each model across runs.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'run', 'model', and the metric columns.\n",
    "        metric_cols (list of str): List of metric column names to plot (e.g., ['macro_f1', 'accuracy'])\n",
    "    \"\"\"\n",
    "    required_cols = {'run', 'model'}\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"DataFrame is missing required columns: {missing}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for model_name, group in df.groupby('model'):\n",
    "        for metric in metric_cols:\n",
    "            if metric not in group.columns:\n",
    "                print(f\"âš ï¸ Skipping: '{metric}' not found in DataFrame for model '{model_name}'\")\n",
    "                continue\n",
    "            label = f\"{model_name} - {metric}\"\n",
    "            plt.plot(group['run'], group[metric], marker='o', label=label)\n",
    "\n",
    "    plt.title('Model Comparison Over Runs')\n",
    "    plt.xlabel('Run')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    if path:\n",
    "        plt.savefig(f'plots/{path}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['run', 'macro_f1', 'micro_f1', 'macro_precision', 'micro_precision',\n",
       "       'macro_recall', 'micro_recall', 'accuracy', 'model'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_res_1 = pd.read_csv('results/cnn_overall_results.csv')\n",
    "cl_res_1['model'] = 'cnn'\n",
    "cl_res_2 = pd.read_csv('results/tcn_overall_results.csv')\n",
    "cl_res_2['model'] = 'tcn'\n",
    "cl_res_3 = pd.read_csv('results/lstm_overall_results.csv')\n",
    "cl_res_3['model'] = 'lstm'\n",
    "\n",
    "cl_total_res = pd.concat([cl_res_1,cl_res_2,cl_res_3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models_combined(cl_total_res,['macro_f1'] ,'Macrof1contlearning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_models_combined(cl_total_res,['micro_f1'] ,'Macrof1contlearning')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
